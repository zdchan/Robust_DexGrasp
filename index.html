<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Robust Dexterous Grasping of General Objects from Single-view Perception.">
  <meta name="keywords" content="RobustDexGrasp">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Robust Dexterous Grasping of General Objects from Single-view Perception</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-72PW1FZDE4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-72PW1FZDE4');
  </script>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RJ93D4865V"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-RJ93D4865V');
  </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Robust Dexterous Grasping of General Objects from Single-view Perception</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zdchan.github.io/">Hui Zhang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://zijianwu1121.github.io/">Zijian Wu</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=boUC8sYAAAAJ&hl=en">Linyi Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/sammyc">Sammy Christen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/SONG-Jie/jsongroas">Jie Song</a><sup>2,3</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ETH ZÃ¼rich, Switzerland</span>
            <!-- <p>&nbsp;</p> -->
            <!-- <br> -->
            <span class="author-block"><sup>2</sup>HKUST (Guangzhou), China</span>
            <!-- <p>&nbsp;</p> -->
            <!-- <br> -->
            <span class="author-block"><sup>3</sup>HKUST, Hong Kong (China)</span>
          </div>

          <div class="is-size-5">
              *Equal Contribution
          </div>
          
<!--            <div class="is-size-5">
              <span class="author-block">  <b>Accepted by ECCV 2024</b></span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zdchan/RobustDexGrasp"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section_video">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-video">
          <video id="teaser" autoplay controls muted loop playsinline height="100%">
            <source src="teaser.mp4"
                    type="video/mp4">
          </video>
        </div>
        <h2 class="subtitle has-text-centered">
            <strong>Relying on single-view perception, our method achieves robust grasping of 500+ unseen objects with various shapes, sizes, materials, masses, and random poses.</strong>
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="background-color: #f5f5f5; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <p>
            Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 87.6% across 247,786 simulated objects and 85.3% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Objects Used for Evaluation</h3>
    <h2 class="subtitle has-text-centered" style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 2rem; margin-top: 2rem;">
      Our method is evaluated on a diverse set of objects with various materials, shapes, and masses, which all unseen during training.
    </h2>
    <!-- <div class="content has-text-justified" style="background-color: #f5f5f5; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      <p>
        Our method is evaluated on a diverse set of objects with various materials, shapes, and masses, which all unseen during training.
      </p>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="has-text-centered">
          <div class="content">
            <img src="objects.png" alt="Diverse set of test objects" style="max-width: 100%; margin: 0 auto; border-radius: 8px;">
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          500+ real-world objects used in our experiments.
        </h2>

        <div class="has-text-centered">
          <div class="content">
            <img src="attributes.jpg" alt="Object attributes" style="max-width: 100%; margin: 2rem auto 0; border-radius: 8px;">
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          Distribution of object attributes.
        </h2>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  <h3 class="title is-3 has-text-centered">Generalization</h3>
  <h2 class="subtitle has-text-centered" style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 2rem; margin-top: 2rem;">
    Trained with only 20 objects in simulation, our method demonstrates exceptional generalization capability across <strong>500+ unseen real objects</strong> with various <strong>physical properties</strong> and <strong>random poses</strong>.
  </h2>
  <!-- <div class="content has-text-justified" style="background-color: #f5f5f5; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p>
      Trained with only 20 objects in simulation, our method demonstrates exceptional generalization capability across <strong>500+ unseen real objects</strong> with various physical properties and random poses.
    </p>
  </div> -->
  <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="has-text-centered">
          <div class="content">
            <video id="unzip_video2" autoplay controls muted loop playsinline height="100%" style="max-width: 80%; margin: 0 auto;">
          <source src="properties.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          <strong>Continual</strong> robust grasping of diverse objects with varying <strong>shapes</strong>, <strong>weights</strong>, and <strong>materials</strong>.
        </h2>

        <div class="has-text-centered">
          <div class="content">
            <video id="unzip_video2" autoplay controls muted loop playsinline height="100%" style="max-width: 80%; margin: 0 auto;">
          <source src="pose.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
        Robust grasping of the same objects with <strong>random poses</strong> on the table.
        </h2>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  <h3 class="title is-3 has-text-centered">Robustness</h3>
  <h2 class="subtitle has-text-centered" style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 2rem; margin-top: 2rem;">
    Utilizing RL, our method shows great <strong>robustness</strong> and <strong>adaptability</strong> to internal & external disturbances.
  </h2>
  <!-- <div class="content has-text-justified" style="background-color: #f5f5f5; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p>
      Utilizing RL, our method shows great robustness and adaptability to internal & external disturbances.
    </p>
  </div> -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="has-text-centered">
          <div class="content">
            <video id="multimat-video" autoplay controls muted loop playsinline height="60%" style="max-width: 80%; margin: 0 auto;">
              <source src="internal.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          Adaptive motions to <strong>observation noises</strong> and <strong>actual inaccuracies</strong>.
        </h2>
      
        <div class="has-text-centered">
          <div class="content">
            <video id="multimat-video2" autoplay controls muted loop playsinline height="100%" style="max-width: 80%; margin: 0 auto;">
              <source src="movement.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          Real-time adaptation to environmental changes such as <strong>object movements</strong>.
        </h2>

        <div class="has-text-centered">
          <div class="content">
            <video id="unzip_video2" autoplay controls muted loop playsinline height="100%" style="max-width: 80%; margin: 0 auto;">
              <source src="force.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          Maintaining stable grasps despite external disturbances such as <strong>unexpected forces</strong>.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  <h3 class="title is-3 has-text-centered">Application</h3>    
  <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="has-text-centered">
          <div class="content">
            <video id="unzip_video2" autoplay controls muted loop playsinline height="100%" style="max-width: 80%; margin: 0 auto;">
              <source src="clustered.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-centered">
          Grasping target objects under the disturbances caused by other objects in a <strong>cluttered environment</strong>.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inProceedings{zhang2025RobustDexGrasp,
  title={Robust Dexterous Grasping of General Objects from Single-view Perception},
  author={Zhang, Hui and Wu, Zijian and Huang, Linyi and Christen, Sammy and Song, Jie},
  booktitle={Arxiv},
  year={2025}
}</code></pre>
  </div>
</section>

</body>
</html>
